# -*- coding: utf-8 -*-
"""Churn_Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13mgFFVkftc_u5l1Xt-DzbHDY1Ha7EQeY
"""

#Mounting the drive or connecting the google drive to colab
from google.colab import drive
drive.mount('/content/drive')

#Instaling the keras libary
!pip install -q keras

#Importing the required Libary
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#importing the dataset
dataset=pd.read_csv('/content/drive/My Drive/Churn_Modelling.csv')

#Seeing the first 5 entries
dataset.head()

#Spliting the dependent and indepented variables
x=dataset.iloc[:,3:13]
y=dataset.iloc[:,13]

#Converting categorical data to numeric categorical data
geography=pd.get_dummies(x["Geography"],drop_first=True)
gender=pd.get_dummies(x["Gender"],drop_first=True)

#joining dummy variable to the dataset
x=pd.concat([x,geography,gender],axis=1)

#dropping the unused data variables 
x=x.drop(['Geography','Gender'],axis=1)

#Diving the train and test data
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3 ,random_state=0)

#Standardizing the variable making into one scale
from sklearn.preprocessing import StandardScaler
standard=StandardScaler()
X_train=standard.fit_transform(x_train)
X_test=standard.fit(x_test)

#importing the keras libaries
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LeakyReLU
from keras.layers import Dropout

#loading the Model
classifier=Sequential()

#Adding the Hidden layers
classifier.add(Dense(units=6,kernel_initializer="he_uniform",activation='relu',input_dim=11))
classifier.add(Dense(units=6,kernel_initializer="he_uniform",activation='relu'))

#Adding the output layer
classifier.add(Dense(units=1,kernel_initializer="glorot_uniform",activation='sigmoid'))

#Compling the model
classifier.compile(optimizer="adam",metrics=['accuracy'],loss="binary_crossentropy")

#Fitting the model
model_history=classifier.fit(X_train,y_train,batch_size=10,validation_split=.33,nb_epoch=100)

print(model_history.history.keys())

y_pred = classifier.predict(x_test)
y_pred = (y_pred > 0.5)

y_pred

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm

from sklearn.metrics import accuracy_score
score=accuracy_score(y_pred,y_test)
score

##By hyperparameter tunning and finding the no of hidden units required in the neural network and what is the best activation function and weight 
##initalizer

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

from keras.models import Sequential
from keras.layers import Dense,Activation,Flatten,Embedding,LeakyReLU,BatchNormalization
from keras.activations import relu,sigmoid
from keras.layers import Dropout

def create_model(layers, activation):
    model = Sequential()
    for i, nodes in enumerate(layers):
        if i==0:
            model.add(Dense(nodes,input_dim=X_train.shape[1]))
            model.add(Activation(activation))
            model.add(Dropout(0.3))
        else:
            model.add(Dense(nodes))
            model.add(Activation(activation))
            model.add(Dropout(0.3))
            
    model.add(Dense(units = 1, kernel_initializer= 'glorot_uniform', activation = 'sigmoid')) # Note: no activation beyond this point
    
    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, verbose=0)

layers = [[20], [40, 20], [45, 30, 15]]
activations = ['sigmoid', 'relu']
param_grid = dict(layers=layers, activation=activations, batch_size = [128, 256], epochs=[30])
grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=5)

grid_result = grid.fit(X_train, y_train)

